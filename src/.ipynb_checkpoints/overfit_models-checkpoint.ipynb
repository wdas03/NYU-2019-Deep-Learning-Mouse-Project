{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_utility import *\n",
    "from model import *\n",
    "from loss import *\n",
    "from sync_batchnorm import convert_model\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU for training\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "NUM_WORKERS = 6\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "dtype = torch.float32 \n",
    "# define dtype, float is space efficient than double\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    # magic flag that accelerate\n",
    "    \n",
    "    print('using GPU for training')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('using CPU for training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* class down_unit\n",
    "    * pass\n",
    "* class up_unit\n",
    "    * pass\n",
    "* class input_unit\n",
    "    * pass\n",
    "* class output_unit\n",
    "    * pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pyramid_dataset(data_type = 'nii_train', \n",
    "                transform=transforms.Compose([\n",
    "                random_affine(90, 15),\n",
    "                random_filp(0.5)]))\n",
    "# do data augumentation on train dataset\n",
    "\n",
    "validation_dataset = pyramid_dataset(data_type = 'nii_test', \n",
    "                transform=None)\n",
    "# no data augumentation on validation dataset\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                    num_workers=NUM_WORKERS)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                    num_workers=NUM_WORKERS)\n",
    "# loaders come with auto batch division and multi-thread acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv3d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "    elif isinstance(m, nn.BatchNorm3d):\n",
    "        init.normal_(m.weight.data, mean=0, std=1)\n",
    "\n",
    "def downsample_label(label, scale_factor):\n",
    "    return F.interpolate(label, scale_factor=scale_factor, mode='trilinear', align_corners=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 1: torch.Size([1, 3, 256, 256, 256]) Downsampled 1/4: torch.Size([1, 3, 64, 64, 64]) Downsampled 1/2: torch.Size([1, 3, 128, 128, 128])\n",
      "Label 2: torch.Size([1, 3, 128, 128, 128]) Downsampled 1/4: torch.Size([1, 3, 32, 32, 32]) Downsampled 1/2: torch.Size([1, 3, 64, 64, 64])\n",
      "Label 4: torch.Size([1, 3, 64, 64, 64]) Downsampled 1/4: torch.Size([1, 3, 16, 16, 16]) Downsampled 1/2: torch.Size([1, 3, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "test_dictionary = train_dataset[33]\n",
    "\n",
    "image_4 = test_dictionary['image4_data'].view(1, 1, 64, 64, 64)\n",
    "label_4 = test_dictionary['image4_label'].view(1, 3, 64, 64, 64)\n",
    "\n",
    "image_2 = test_dictionary['image2_data'].view(1, 1, 128, 128, 128)\n",
    "label_2 = test_dictionary['image2_label'].view(1, 3, 128, 128, 128)\n",
    "\n",
    "image_1 = test_dictionary['image1_data'].view(1, 1, 256, 256, 256)\n",
    "label_1 = test_dictionary['image1_label'].view(1, 3, 256, 256, 256)\n",
    "\n",
    "label_1_resize_2 = downsample_label(label_1, 1/2) \n",
    "label_2_resize_2 = downsample_label(label_2, 1/2)\n",
    "label_4_resize_2 = downsample_label(label_4, 1/2)\n",
    "\n",
    "label_1_resize_4 = downsample_label(label_1, 1/4) \n",
    "label_2_resize_4 = downsample_label(label_2, 1/4)\n",
    "label_4_resize_4 = downsample_label(label_4, 1/4)\n",
    "\n",
    "image_4 = image_4.to(device=device, dtype=dtype)  # move to device, fix dtype\n",
    "label_4 = label_4.to(device=device, dtype=dtype)\n",
    "label_4_resize_4 = label_4_resize_4.to(device=device, dtype=dtype)\n",
    "label_4_resize_2 = label_4_resize_2.to(device=device, dtype=dtype)\n",
    "\n",
    "image_2 = image_2.to(device=device, dtype=dtype)\n",
    "label_2 = label_2.to(device=device, dtype=dtype)\n",
    "label_2_resize_4 = label_2_resize_4.to(device=device, dtype=dtype)\n",
    "label_2_resize_2 = label_2_resize_2.to(device=device, dtype=dtype)\n",
    "\n",
    "image_1 = image_1.to(device=device, dtype=dtype) \n",
    "label_1 = label_1.to(device=device, dtype=dtype)\n",
    "label_1_resize_4 = label_1_resize_4.to(device=device, dtype=dtype)\n",
    "label_1_resize_2 = label_1_resize_2.to(device=device, dtype=dtype)\n",
    "\n",
    "print(\"Label 1:\", label_1.shape, \"Downsampled 1/4:\", label_1_resize_4.shape, \"Downsampled 1/2:\", label_1_resize_2.shape)\n",
    "print(\"Label 2:\", label_2.shape, \"Downsampled 1/4:\", label_2_resize_4.shape, \"Downsampled 1/2:\", label_2_resize_2.shape)\n",
    "print(\"Label 4:\", label_4.shape, \"Downsampled 1/4:\", label_4_resize_4.shape, \"Downsampled 1/2:\", label_4_resize_2.shape)\n",
    "\n",
    "from model import *\n",
    "\n",
    "icnet1 = ModifiedICNet(num_classes=3)\n",
    "icnet1.apply(init_weights)\n",
    "icnet1 = icnet1.to(device=device, dtype=dtype)\n",
    "\n",
    "icnet2 = OriginalICNet(num_classes=3)\n",
    "icnet2.apply(init_weights)\n",
    "icnet2 = icnet2.to(device=device, dtype=dtype)\n",
    "\n",
    "full_res_icnet = FullResolutionICNet(num_classes=3)\n",
    "full_res_icnet = nn.DataParallel(full_res_icnet)\n",
    "full_res_icnet = convert_model(full_res_icnet)\n",
    "full_res_icnet.apply(init_weights)\n",
    "full_res_icnet = full_res_icnet.to(device=device, dtype=dtype)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer1 = optim.Adam(icnet1.parameters(), lr=1e-2)\n",
    "optimizer2 = optim.Adam(icnet2.parameters(), lr=1e-2)\n",
    "optimizer3 = optim.Adam(full_res_icnet.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from model import *\n",
    "\n",
    "def shape_test(model, cuda_bool):\n",
    "    x = torch.zeros((1, 1, 256, 256, 256))\n",
    "    x = x.to(device=device, dtype=dtype) if cuda_bool else x\n",
    "    scores = model(x)\n",
    "    for i in scores:\n",
    "        print(i.size())\n",
    "\n",
    "m = FullResolutionICNet(num_classes=3)\n",
    "m = nn.DataParallel(m)\n",
    "m = convert_model(m)\n",
    "m = m.to(device=device, dtype=dtype)\n",
    "shape_test(m, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* network\n",
    "    * test with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfit model on single embryo image (modified ICNet Model)\n",
    "# upsample final outputs by a factor of 4 instead of factor 2\n",
    "import datetime\n",
    "from loss import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 5000\n",
    "\n",
    "record = open('over_fit_modified_model.txt','w+')\n",
    "\n",
    "for e in tqdm(range(epochs)):\n",
    "    \n",
    "    out_1, out_2, out_4 = icnet1(image_1)\n",
    "        \n",
    "    loss_4 = dice_loss_3(out_4, label_4_resize_2)\n",
    "    loss_2 = dice_loss_3(out_2, label_2_resize_2)\n",
    "    loss_1 = dice_loss_3(out_1, label_1_resize_2)\n",
    "    \n",
    "    #loss = loss_4 + loss_2 + loss_1 \n",
    "    loss = loss_1 + loss_2 + loss_4\n",
    "    \n",
    "    outstr = 'in epoch {}, loss = {}, loss_1: {}'.format(e, loss.item(), loss_1.item()) + '\\n'\n",
    "    \n",
    "    print(outstr) \n",
    "    record.write(outstr)\n",
    "    record.flush()\n",
    "    \n",
    "    optimizer1.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer1.step()\n",
    "\n",
    "record.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# overfit model on single embryo image (original ICNet Model)\n",
    "\n",
    "from loss import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 5000\n",
    "\n",
    "record = open('over_fit_original_model.txt','w+')\n",
    "\n",
    "for e in tqdm(range(epochs)):\n",
    "    \n",
    "    out_1, out_2, out_4 = icnet2(image_1)\n",
    "        \n",
    "    loss_4 = dice_loss_3(out_4, label_4_resize_4)\n",
    "    loss_2 = dice_loss_3(out_2, label_2_resize_4)\n",
    "    loss_1 = dice_loss_3(out_1, label_1_resize_4)\n",
    "    \n",
    "    #loss = loss_4 + loss_2 + loss_1 \n",
    "    loss = loss_4 + loss_2 + loss_1\n",
    "    \n",
    "    outstr = 'in epoch {}, loss = {}, loss_1: {}, loss_2: {}, loss_4: {}'.format(e, loss.item(), loss_1.item(), loss_2.item(), loss_4.item()) + '\\n'\n",
    "    \n",
    "    print(outstr) \n",
    "    record.write(outstr)\n",
    "    record.flush()\n",
    "    \n",
    "    optimizer2.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "\n",
    "record.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv5_4_interp: torch.Size([1, 64, 64, 64, 64])\n",
      "conv3_1_sub2_proj: torch.Size([1, 32, 16, 16, 16])\n",
      "conv_sub4: torch.Size([1, 32, 16, 16, 16])\n",
      "conv_sub2: torch.Size([1, 32, 64, 64, 64])\n",
      "x: torch.Size([1, 1, 256, 256, 256])\n",
      "conv1_sub1: torch.Size([1, 8, 256, 256, 256])\n",
      "conv2_sub1: torch.Size([1, 8, 256, 256, 256])\n",
      "conv3_sub1: torch.Size([1, 16, 256, 256, 256])\n",
      "conv_sub2: torch.Size([1, 32, 64, 64, 64])\n",
      "conv3_sub1_proj: torch.Size([1, 32, 128, 128, 128])\n",
      "in epoch 0, loss = 2.251490592956543, loss_1: 0.7215265035629272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overfit model on single embryo image (modified ICNet Model)\n",
    "# upsample final outputs by a factor of 4 instead of factor 2\n",
    "import datetime\n",
    "from loss import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 5000\n",
    "\n",
    "record = open('over_fit_full_res_model_check2.txt','w+')\n",
    "\n",
    "for e in tqdm(range(epochs)):\n",
    "    \n",
    "    out_1, out_2, out_4 = full_res_icnet(image_1)\n",
    "        \n",
    "    #loss_4 = dice_loss_3(out_4, label_4_resize_2)\n",
    "    #loss_2 = dice_loss_3(out_2, label_2_resize_2)\n",
    "    loss_4 = dice_loss_3(out_4, label_4)\n",
    "    loss_2 = dice_loss_3(out_2, label_2)\n",
    "    loss_1 = dice_loss_3(out_1, label_1)\n",
    "    \n",
    "    #loss = loss_4 + loss_2 + loss_1 \n",
    "    loss = loss_1 + loss_2 + loss_4\n",
    "    \n",
    "    outstr = 'in epoch {}, loss = {}, loss_1: {}, loss_2: {}'.format(e, loss.item(), loss_1.item(), loss_2.item()) + '\\n'\n",
    "    \n",
    "    print(outstr) \n",
    "    record.write(outstr)\n",
    "    record.flush()\n",
    "    \n",
    "    optimizer3.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer3.step()\n",
    "\n",
    "record.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfit model on single embryo image (modified ICNet Model)\n",
    "# upsample final outputs by a factor of 4 instead of factor 2\n",
    "import datetime\n",
    "from loss import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 5000\n",
    "\n",
    "record = open('over_fit_deeplab.txt','w+')\n",
    "\n",
    "for e in tqdm(range(epochs)):\n",
    "    \n",
    "    out_1 = deeplab(image_1)\n",
    "        \n",
    "    \n",
    "    loss_1 = dice_loss_3(out_1, label_1)\n",
    "    \n",
    "    #loss = loss_4 + loss_2 + loss_1 \n",
    "    loss = loss_1\n",
    "    \n",
    "    outstr = 'in epoch {}, loss = {}'.format(e, loss.item()) + '\\n'\n",
    "    \n",
    "    print(outstr) \n",
    "    record.write(outstr)\n",
    "    record.flush()\n",
    "    \n",
    "    optimizer1.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer1.step()\n",
    "\n",
    "record.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
