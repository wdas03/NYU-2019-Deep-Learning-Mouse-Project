{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_utility import *\n",
    "from model_dirty import *\n",
    "from loss import *\n",
    "from deeplab_model.deeplab import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU for training\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "NUM_WORKERS = 12\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "dtype = torch.float32 \n",
    "# define dtype, float is space efficient than double\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    # magic flag that accelerate\n",
    "    \n",
    "    print('using GPU for training')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('using CPU for training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_test(model, cuda_bool):\n",
    "    x = torch.zeros((1, 1, 256, 256, 256))\n",
    "    x = x.to(device=device, dtype=dtype) if cuda_bool else x\n",
    "    if cuda_bool:\n",
    "        model = model.to(device=device, dtype=dtype)\n",
    "    scores = model(x)\n",
    "    for i in scores:\n",
    "        print(i.size())\n",
    "\n",
    "#m = DeepLab()\n",
    "#shape_test(m, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pyramid_dataset(data_type = 'nii_train', \n",
    "                transform=transforms.Compose([\n",
    "                random_affine(90, 15),\n",
    "                random_filp(0.5)]))\n",
    "# do data augumentation on train dataset\n",
    "\n",
    "validation_dataset = pyramid_dataset(data_type = 'nii_test', \n",
    "                transform=None)\n",
    "# no data augumentation on validation dataset\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                    num_workers=NUM_WORKERS)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                    num_workers=NUM_WORKERS)\n",
    "# loaders come with auto batch division and multi-thread acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dictionary = train_dataset[33]\n",
    "\n",
    "image_1 = test_dictionary['image1_data'].view(1, 1, 256, 256, 256)\n",
    "label_1 = test_dictionary['image1_label'].view(1, 3, 256, 256, 256)\n",
    "\n",
    "image_1 = image_1.to(device=device, dtype=dtype) \n",
    "label_1 = label_1.to(device=device, dtype=dtype)\n",
    "\n",
    "deeplab = DeepLab()\n",
    "deeplab = deeplab.to(device=device, dtype=dtype)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer_deeplab = optim.Adam(deeplab.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1304, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1304, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(941260., device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test = deeplab(image_1)\n",
    "print(focal_loss_categorical(test, label_1))\n",
    "print(focal_loss_3(test, label_1))\n",
    "#print(focal_loss_test(test, label_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfit model on single embryo image (modified ICNet Model)\n",
    "# upsample final outputs by a factor of 4 instead of factor 2\n",
    "import datetime\n",
    "from loss import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 5000\n",
    "\n",
    "record = open('over_fit_deeplab_focal_loss.txt','w+')\n",
    "\n",
    "for e in tqdm(range(epochs)):\n",
    "    \n",
    "    out_1 = deeplab(image_1)\n",
    "    \n",
    "    #loss_1 = dice_loss_3(out_1, label_1)\n",
    "    loss_1 = focal_loss_categorical(out_1, label_1)\n",
    "    #loss = loss_4 + loss_2 + loss_1 \n",
    "    loss = loss_1\n",
    "    \n",
    "    outstr = 'in epoch {}, loss = {}, dice = {}'.format(e, loss.item(), dice_loss_3(out_1, label_1).item()) + '\\n'\n",
    "    \n",
    "    print(outstr) \n",
    "    record.write(outstr)\n",
    "    record.flush()\n",
    "    \n",
    "    optimizer_deeplab.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_deeplab.step()\n",
    "\n",
    "record.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
